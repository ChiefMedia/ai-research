{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "353054ca",
   "metadata": {},
   "source": [
    "# Research with Gemini\n",
    "\n",
    "We're going to be testing Google's Gemini API. \n",
    "\n",
    "Credentials are located in config.yaml in the ai-research root folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1599ee",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e888abe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from chiefai.ai import make_gemini_request\n",
    "from chiefai.db import query\n",
    "import polars as pl\n",
    "\n",
    "# Notebook formatting\n",
    "from IPython.display import display, HTML, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f6420d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 37)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>core_client</th><th>project_id</th><th>user_id</th><th>session_id</th><th>unique_key</th><th>postal_code</th><th>region</th><th>dma</th><th>dma_code</th><th>city</th><th>country</th><th>browser</th><th>device</th><th>device_type</th><th>search_engine</th><th>medium</th><th>source</th><th>platform</th><th>platform_version</th><th>bounce</th><th>session_date_time</th><th>ip_address</th><th>pages</th><th>search_terms</th><th>language</th><th>latitude</th><th>longitude</th><th>organization</th><th>referrer</th><th>session_length</th><th>created_at</th><th>updated_at</th><th>core_product</th><th>last_access</th><th>content</th><th>session</th></tr><tr><td>i64</td><td>str</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>datetime[μs]</td><td>str</td><td>list[struct[2]]</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>datetime[μs]</td><td>datetime[μs]</td><td>str</td><td>datetime[μs]</td><td>null</td><td>null</td></tr></thead><tbody><tr><td>99735578</td><td>&quot;OPOS&quot;</td><td>82884616</td><td>&quot;50182511673882&quot;</td><td>&quot;3288761085059530754&quot;</td><td>&quot;828846165018251167388232887610…</td><td>null</td><td>null</td><td>null</td><td>0</td><td>null</td><td>&quot;USA&quot;</td><td>&quot;Unknown&quot;</td><td>null</td><td>&quot;Desktop/laptop&quot;</td><td>null</td><td>&quot;paidsocial&quot;</td><td>&quot;snapchat&quot;</td><td>&quot;Unknown&quot;</td><td>&quot;Unknown&quot;</td><td>&quot;true&quot;</td><td>2024-01-03 13:11:13</td><td>&quot;34.123.204.87&quot;</td><td>[{&quot;collections/vaginal-health&quot;,0}]</td><td>null</td><td>&quot;Unknown&quot;</td><td>37.751</td><td>-97.822</td><td>&quot;Google&quot;</td><td>&quot;Direct&quot;</td><td>&quot;0&quot;</td><td>2024-01-03 13:13:00.337214</td><td>2024-01-03 13:13:00.337784</td><td>&quot;&quot;</td><td>2024-01-03 13:11:13</td><td>null</td><td>null</td></tr><tr><td>99735579</td><td>&quot;OPOS&quot;</td><td>82884616</td><td>&quot;78135251450464&quot;</td><td>&quot;5120671839057608705&quot;</td><td>&quot;828846167813525145046451206718…</td><td>&quot;32163&quot;</td><td>&quot;Florida&quot;</td><td>&quot;Orlando, FL&quot;</td><td>534</td><td>&quot;The Villages&quot;</td><td>&quot;USA&quot;</td><td>&quot;Safari&quot;</td><td>&quot;Apple iPhone&quot;</td><td>&quot;Mobile&quot;</td><td>null</td><td>&quot;paidsocial&quot;</td><td>&quot;ig&quot;</td><td>&quot;iOS&quot;</td><td>&quot;iOS 17.1&quot;</td><td>&quot;true&quot;</td><td>2024-01-03 13:11:13</td><td>&quot;68.205.39.5&quot;</td><td>[{&quot;collections/vaginal-health&quot;,0}]</td><td>null</td><td>&quot;English (United States)&quot;</td><td>28.9265</td><td>-81.9928</td><td>&quot;Spectrum&quot;</td><td>&quot;instagram.com&quot;</td><td>&quot;0&quot;</td><td>2024-01-03 13:13:00.337214</td><td>2024-01-03 13:13:00.337784</td><td>&quot;&quot;</td><td>2024-01-03 13:11:13</td><td>null</td><td>null</td></tr><tr><td>99735580</td><td>&quot;OPOS&quot;</td><td>82884616</td><td>&quot;103390995781948&quot;</td><td>&quot;6775832299565744285&quot;</td><td>&quot;828846161033909957819486775832…</td><td>null</td><td>null</td><td>null</td><td>0</td><td>null</td><td>&quot;USA&quot;</td><td>&quot;Chrome&quot;</td><td>&quot;Generic Android&quot;</td><td>&quot;Mobile&quot;</td><td>null</td><td>null</td><td>&quot;Direct&quot;</td><td>&quot;Android&quot;</td><td>&quot;Android 9.0&quot;</td><td>&quot;true&quot;</td><td>2024-01-03 13:11:13</td><td>&quot;147.160.184.123&quot;</td><td>[{&quot;tools/recurring/login&quot;,0}]</td><td>null</td><td>&quot;English (United States)&quot;</td><td>37.751</td><td>-97.822</td><td>&quot;Unknown&quot;</td><td>&quot;Direct&quot;</td><td>&quot;0&quot;</td><td>2024-01-03 13:13:00.337214</td><td>2024-01-03 13:13:00.337784</td><td>&quot;&quot;</td><td>2024-01-03 13:11:13</td><td>null</td><td>null</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 37)\n",
       "┌──────────┬────────────┬────────────┬────────────┬───┬────────────┬───────────┬─────────┬─────────┐\n",
       "│ id       ┆ core_clien ┆ project_id ┆ user_id    ┆ … ┆ core_produ ┆ last_acce ┆ content ┆ session │\n",
       "│ ---      ┆ t          ┆ ---        ┆ ---        ┆   ┆ ct         ┆ ss        ┆ ---     ┆ ---     │\n",
       "│ i64      ┆ ---        ┆ i64        ┆ str        ┆   ┆ ---        ┆ ---       ┆ null    ┆ null    │\n",
       "│          ┆ str        ┆            ┆            ┆   ┆ str        ┆ datetime[ ┆         ┆         │\n",
       "│          ┆            ┆            ┆            ┆   ┆            ┆ μs]       ┆         ┆         │\n",
       "╞══════════╪════════════╪════════════╪════════════╪═══╪════════════╪═══════════╪═════════╪═════════╡\n",
       "│ 99735578 ┆ OPOS       ┆ 82884616   ┆ 5018251167 ┆ … ┆            ┆ 2024-01-0 ┆ null    ┆ null    │\n",
       "│          ┆            ┆            ┆ 3882       ┆   ┆            ┆ 3         ┆         ┆         │\n",
       "│          ┆            ┆            ┆            ┆   ┆            ┆ 13:11:13  ┆         ┆         │\n",
       "│ 99735579 ┆ OPOS       ┆ 82884616   ┆ 7813525145 ┆ … ┆            ┆ 2024-01-0 ┆ null    ┆ null    │\n",
       "│          ┆            ┆            ┆ 0464       ┆   ┆            ┆ 3         ┆         ┆         │\n",
       "│          ┆            ┆            ┆            ┆   ┆            ┆ 13:11:13  ┆         ┆         │\n",
       "│ 99735580 ┆ OPOS       ┆ 82884616   ┆ 1033909957 ┆ … ┆            ┆ 2024-01-0 ┆ null    ┆ null    │\n",
       "│          ┆            ┆            ┆ 81948      ┆   ┆            ┆ 3         ┆         ┆         │\n",
       "│          ┆            ┆            ┆            ┆   ┆            ┆ 13:11:13  ┆         ┆         │\n",
       "└──────────┴────────────┴────────────┴────────────┴───┴────────────┴───────────┴─────────┴─────────┘"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = query(\"SELECT * FROM web_visit_results LIMIT 5\")\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87173c2c",
   "metadata": {},
   "source": [
    "## Assess AI Approach to Data Analysis\n",
    "\n",
    "We're going to use the make_gemini_request function in chiefmedai.ai to send data. \n",
    "\n",
    "The approach we're going to try is to create summary statistics from the database and programmatically generate a text query to feed to the AI model. We'll use prompt injection of the data to do this. \n",
    "\n",
    "Firstly we'll try getting simple counts of orders by platform and ask the AI model to assess the results. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0d42203",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = \"\"\"\n",
    "WITH results AS (\n",
    "    select \n",
    "        p.unique_key,\n",
    "        COALESCE(s.station_name, p.station) AS station, \n",
    "        p.date,\n",
    "        CASE \n",
    "            WHEN EXTRACT(ISODOW FROM p.date) = 1 THEN 'Monday'\n",
    "            WHEN EXTRACT(ISODOW FROM p.date) = 2 THEN 'Tuesday'\n",
    "            WHEN EXTRACT(ISODOW FROM p.date) = 3 THEN 'Wednesday'\n",
    "            WHEN EXTRACT(ISODOW FROM p.date) = 4 THEN 'Thursday'\n",
    "            WHEN EXTRACT(ISODOW FROM p.date) = 5 THEN 'Friday'\n",
    "            WHEN EXTRACT(ISODOW FROM p.date) = 6 THEN 'Saturday'\n",
    "            ELSE 'Sunday'\n",
    "        END AS weekday,\n",
    "        cbd.cdaypart AS daypart, \n",
    "        p.length, \n",
    "        p.buyrate AS spend, \n",
    "        lam.online_visits, \n",
    "        lam.online_orders, \n",
    "        lam.online_revenue, \n",
    "        lam.online_leads, \n",
    "        lam.impressions AS target_demo_impressions, \n",
    "        lam.impressions2 AS total_impressions\n",
    "    from core_post_time p\n",
    "    left outer join core_tape_details td ON td.tapecd = p.tape\n",
    "        AND td.cmedia = p.media\n",
    "        AND td.cclient = p.client\n",
    "        AND td.cproduct = p.product\n",
    "        AND td.startdate <= p.bcdate\n",
    "        AND td.enddate >= p.bcdate\n",
    "    left outer join core_tape_parent tpp ON tpp.ctpparent = td.ctpparent\n",
    "        AND tpp.cclient = p.client\n",
    "        AND tpp.cproduct = p.product\n",
    "    left outer join linear_attribution_metrics lam ON lam.unique_key = p.unique_key\n",
    "    join core_estimate est ON est.cmedia = p.media\n",
    "        AND est.cclient = p.client\n",
    "        AND est.cproduct = p.product\n",
    "        AND est.cestimate = p.estimate\n",
    "    join core_buy_detail cbd ON cbd.nbuydetid = p.buydetid\n",
    "    join core_buy_table cbt ON cbt.nbuyid = p.buyid\n",
    "    join stations s ON s.core_label = p.station\n",
    "    where p.client = 'OPOS'\n",
    "        AND p.media != 'OL'\n",
    "        AND p.date >= NOW() - INTERVAL '4 weeks'\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    station,\n",
    "    weekday,\n",
    "    daypart,\n",
    "    COUNT(unique_key) AS spot_count,\n",
    "    SUM(spend) AS total_spend,\n",
    "    ROUND(AVG(spend)) AS average_spend,\n",
    "    SUM(online_visits) AS total_visits,\n",
    "    ROUND(AVG(online_visits)) AS average_visits,\n",
    "    SUM(online_leads) AS total_leads,\n",
    "    ROUND(AVG(online_leads)) AS average_leads,\n",
    "    SUM(online_orders) AS total_orders,\n",
    "    ROUND(AVG(online_orders)) AS average_orders,\n",
    "    ROUND(SUM(online_revenue)::NUMERIC, 2) AS total_revenue,\n",
    "    ROUND(AVG(online_revenue)::NUMERIC, 2) AS average_order_value,\n",
    "    SUM(target_demo_impressions) AS total_target_demo_impressions,\n",
    "    ROUND(AVG(target_demo_impressions)) AS average_target_demo_impressions,\n",
    "    CASE \n",
    "        WHEN SUM(target_demo_impressions) = 0 THEN 0 \n",
    "        ELSE ROUND(SUM(spend) / SUM(target_demo_impressions), 2)\n",
    "    END AS cpm_target_demo,\n",
    "    SUM(total_impressions) AS total_all_demo_impressions,\n",
    "    ROUND(AVG(total_impressions)) AS average_all_demo_impressions,\n",
    "    CASE \n",
    "        WHEN SUM(total_impressions) = 0 THEN 0 \n",
    "        ELSE ROUND(SUM(spend) / SUM(total_impressions), 2)\n",
    "    END AS cpm_all_demo\n",
    "FROM results\n",
    "GROUP BY     \n",
    "    station,\n",
    "    weekday,\n",
    "    daypart\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d1f0e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (3, 20)\n",
      "┌────────────┬─────────┬─────────┬────────────┬───┬────────────┬───────────┬───────────┬───────────┐\n",
      "│ station    ┆ weekday ┆ daypart ┆ spot_count ┆ … ┆ cpm_target ┆ total_all ┆ average_a ┆ cpm_all_d │\n",
      "│ ---        ┆ ---     ┆ ---     ┆ ---        ┆   ┆ _demo      ┆ _demo_imp ┆ ll_demo_i ┆ emo       │\n",
      "│ str        ┆ str     ┆ str     ┆ i64        ┆   ┆ ---        ┆ ressions  ┆ mpression ┆ ---       │\n",
      "│            ┆         ┆         ┆            ┆   ┆ decimal[*, ┆ ---       ┆ s         ┆ decimal[* │\n",
      "│            ┆         ┆         ┆            ┆   ┆ 2]         ┆ i64       ┆ ---       ┆ ,2]       │\n",
      "│            ┆         ┆         ┆            ┆   ┆            ┆           ┆ decimal[* ┆           │\n",
      "│            ┆         ┆         ┆            ┆   ┆            ┆           ┆ ,0]       ┆           │\n",
      "╞════════════╪═════════╪═════════╪════════════╪═══╪════════════╪═══════════╪═══════════╪═══════════╡\n",
      "│ A&E        ┆ Friday  ┆ DA      ┆ 1          ┆ … ┆ null       ┆ null      ┆ null      ┆ null      │\n",
      "│ NETWORK    ┆         ┆         ┆            ┆   ┆            ┆           ┆           ┆           │\n",
      "│ A&E        ┆ Friday  ┆ EF      ┆ 3          ┆ … ┆ 65.45      ┆ 315       ┆ 105       ┆ 11.43     │\n",
      "│ NETWORK    ┆         ┆         ┆            ┆   ┆            ┆           ┆           ┆           │\n",
      "│ A&E        ┆ Monday  ┆ DA      ┆ 2          ┆ … ┆ 63.64      ┆ 333       ┆ 167       ┆ 4.20      │\n",
      "│ NETWORK    ┆         ┆         ┆            ┆   ┆            ┆           ┆           ┆           │\n",
      "└────────────┴─────────┴─────────┴────────────┴───┴────────────┴───────────┴───────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "data = query(query_str)\n",
    "print(data.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7654e1",
   "metadata": {},
   "source": [
    "### Descriptive Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdf7fb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_descriptive_prompt(\n",
    "    df, \n",
    "    group_cols, \n",
    "    currency_symbol=\"$\", \n",
    "    add_comparisons=True,\n",
    "    decimal_places=2\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a descriptive text prompt from a dataframe with grouped statistics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : polars.DataFrame or pandas.DataFrame\n",
    "        The dataframe containing the grouped statistics\n",
    "    group_cols : list or str\n",
    "        Column name(s) that represent categorical groupings\n",
    "    currency_symbol : str, optional\n",
    "        Symbol to use for currency formatting (default: \"$\")\n",
    "    add_comparisons : bool, optional\n",
    "        Whether to add comparisons between stats of the same measure (default: True)\n",
    "    decimal_places : int, optional\n",
    "        Number of decimal places to round numeric values (default: 2)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Formatted descriptive text suitable for an AI prompt\n",
    "    \"\"\"\n",
    "    # Convert group_cols to list if it's a string\n",
    "    if isinstance(group_cols, str):\n",
    "        group_cols = [group_cols]\n",
    "    \n",
    "    # Validate inputs\n",
    "    for col in group_cols:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Group column '{col}' not found in dataframe\")\n",
    "    \n",
    "    # Infer stat columns (all columns that aren't group columns)\n",
    "    stat_cols = [col for col in df.columns if col not in group_cols]\n",
    "    \n",
    "    # Check if all stat columns are numeric\n",
    "    for col in stat_cols:\n",
    "        # Check column type for both polars and pandas\n",
    "        try:\n",
    "            # For polars\n",
    "            if hasattr(df, 'dtypes'):\n",
    "                col_type = df[col].dtype\n",
    "                is_numeric = any(num_type in str(col_type) for num_type in ['Int', 'Float', 'int', 'float'])\n",
    "            # For pandas\n",
    "            else:\n",
    "                is_numeric = df[col].dtype.kind in 'ifc'\n",
    "                \n",
    "            if not is_numeric:\n",
    "                raise ValueError(f\"Stat column '{col}' must be numeric, but got type {df[col].dtype}\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error checking if column '{col}' is numeric: {str(e)}\")\n",
    "    \n",
    "    # Determine if we're using polars or pandas\n",
    "    is_polars = hasattr(df, 'iter_rows')\n",
    "    \n",
    "    # Get rows for iteration\n",
    "    if is_polars:\n",
    "        rows = list(df.iter_rows(named=True))\n",
    "    else:  # pandas\n",
    "        rows = df.to_dict('records')\n",
    "    \n",
    "    # Group stat columns by their measure\n",
    "    measures = {}\n",
    "    for col in stat_cols:\n",
    "        # Try to split the column name into statistic type and measure\n",
    "        parts = col.split('_', 1)\n",
    "        if len(parts) >= 2:\n",
    "            stat_type, measure = parts[0], parts[1]\n",
    "            if measure not in measures:\n",
    "                measures[measure] = {}\n",
    "            measures[measure][stat_type] = col\n",
    "    \n",
    "    # Start building the prompt\n",
    "    prompt_text = \"\"\n",
    "    \n",
    "    # Format a value based on its measure\n",
    "    def format_value(value, measure):\n",
    "        if value is None:\n",
    "            return \"N/A\"\n",
    "        \n",
    "        value = round(value, decimal_places) if isinstance(value, (int, float)) else value\n",
    "        \n",
    "        # Format based on measure name\n",
    "        if any(term in measure for term in ['revenue', 'price', 'cost', 'income', 'expense']):\n",
    "            return f\"{currency_symbol}{value:.{decimal_places}f}\" if isinstance(value, float) else f\"{currency_symbol}{value}\"\n",
    "        elif any(term in measure for term in ['percent', 'rate', 'ratio']):\n",
    "            return f\"{value:.{decimal_places}f}%\" if isinstance(value, float) else f\"{value}%\"\n",
    "        else:\n",
    "            return f\"{value:.{decimal_places}f}\" if isinstance(value, float) else f\"{value}\"\n",
    "    \n",
    "    for row in rows:\n",
    "        # Create the group description\n",
    "        group_desc = \"\"\n",
    "        for col in group_cols:\n",
    "            label = col.replace('_', ' ').title()\n",
    "            group_desc += f\"{label}: {row[col]}\\n\"\n",
    "        \n",
    "        # Add the statistics descriptions\n",
    "        stats_desc = \"\"\n",
    "        \n",
    "        # Process each measure and its stats\n",
    "        for measure, stats in measures.items():\n",
    "            measure_label = measure.replace('_', ' ')\n",
    "            \n",
    "            # Process each statistic for this measure\n",
    "            for stat_type, col in stats.items():\n",
    "                value = row[col]\n",
    "                formatted_value = format_value(value, measure)\n",
    "                \n",
    "                # Create description based on statistic type\n",
    "                if stat_type == 'count':\n",
    "                    stats_desc += f\"There were {formatted_value} {measure_label}.\\n\"\n",
    "                elif stat_type == 'mean':\n",
    "                    stats_desc += f\"The average (mean) {measure_label} was {formatted_value}.\\n\"\n",
    "                elif stat_type == 'median':\n",
    "                    stats_desc += f\"The median {measure_label} was {formatted_value}.\\n\"\n",
    "                elif stat_type == 'sum' or stat_type == 'total':\n",
    "                    stats_desc += f\"The total {measure_label} was {formatted_value}.\\n\"\n",
    "                elif stat_type == 'min':\n",
    "                    stats_desc += f\"The minimum {measure_label} was {formatted_value}.\\n\"\n",
    "                elif stat_type == 'max':\n",
    "                    stats_desc += f\"The maximum {measure_label} was {formatted_value}.\\n\"\n",
    "                else:\n",
    "                    stats_desc += f\"The {stat_type} of {measure_label} was {formatted_value}.\\n\"\n",
    "            \n",
    "            # Add comparisons if requested and if we have both mean and median\n",
    "            if add_comparisons and 'mean' in stats and 'median' in stats:\n",
    "                mean_value = row[stats['mean']]\n",
    "                median_value = row[stats['median']]\n",
    "                \n",
    "                if mean_value is not None and median_value is not None:\n",
    "                    if mean_value > median_value:\n",
    "                        difference = round(mean_value - median_value, decimal_places)\n",
    "                        formatted_diff = format_value(difference, measure)\n",
    "                        stats_desc += f\"The mean is {formatted_diff} higher than the median, suggesting some high-value outlier {measure_label}.\\n\"\n",
    "                    elif median_value > mean_value:\n",
    "                        difference = round(median_value - mean_value, decimal_places)\n",
    "                        formatted_diff = format_value(difference, measure)\n",
    "                        stats_desc += f\"The median is {formatted_diff} higher than the mean, suggesting some low-value outlier {measure_label}.\\n\"\n",
    "                    else:\n",
    "                        stats_desc += f\"The mean and median are identical, suggesting a symmetrical distribution of {measure_label}.\\n\"\n",
    "        \n",
    "        # Combine group and stats descriptions\n",
    "        section = group_desc + stats_desc + (\"-\" * 40 + \"\\n\")\n",
    "        prompt_text += section\n",
    "    \n",
    "    # Remove the final separator\n",
    "    prompt_text = prompt_text.rstrip(\"-\" * 40 + \"\\n\")\n",
    "    \n",
    "    # Add a summary\n",
    "    groups_count = len(rows)\n",
    "    if len(group_cols) == 1:\n",
    "        group_label = group_cols[0].replace('_', ' ').lower()\n",
    "        prompt_text += f\"\\n\\nSummary: Analyzed statistics for {groups_count} different {group_label}s.\"\n",
    "    else:\n",
    "        group_labels = [col.replace('_', ' ').lower() for col in group_cols]\n",
    "        group_desc = \", \".join(group_labels)\n",
    "        prompt_text += f\"\\n\\nSummary: Analyzed statistics for {groups_count} different groups based on {group_desc}.\"\n",
    "    \n",
    "    return prompt_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be6eb96",
   "metadata": {},
   "source": [
    "### Tabular Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "377c3852",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "def generate_table_prompt(\n",
    "    df,\n",
    "    title=\"Data Analysis\",\n",
    "    description=None,\n",
    "    format_currency=True,\n",
    "    currency_cols=None, \n",
    "    currency_symbol=\"$\",\n",
    "    decimal_places=2,\n",
    "    analysis_request=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate an AI prompt with a markdown table from a Polars dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : polars.DataFrame\n",
    "        The Polars dataframe to convert to a table\n",
    "    title : str, optional\n",
    "        Title to use in the prompt (default: \"Data Analysis\")\n",
    "    description : str, optional\n",
    "        Custom description of the data (default: auto-generated based on columns)\n",
    "    format_currency : bool, optional\n",
    "        Whether to automatically format columns that appear to be currency (default: True)\n",
    "    currency_cols : list, optional\n",
    "        Specific columns to format as currency (default: None, auto-detect)\n",
    "    currency_symbol : str, optional\n",
    "        Symbol to use for currency formatting (default: \"$\")\n",
    "    decimal_places : int, optional\n",
    "        Number of decimal places for numeric values (default: 2)\n",
    "    analysis_request : str or list, optional\n",
    "        Custom analysis instructions (default: standard analysis request)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        A prompt with the data formatted as a markdown table\n",
    "    \"\"\"\n",
    "    # Create a copy of the dataframe to avoid modifying the original\n",
    "    formatted_df = df.clone()\n",
    "    \n",
    "    # Format currency columns if requested\n",
    "    if format_currency:\n",
    "        # Auto-detect currency columns if not specified\n",
    "        if currency_cols is None:\n",
    "            currency_cols = []\n",
    "            for col in formatted_df.columns:\n",
    "                col_lower = str(col).lower()\n",
    "                if any(term in col_lower for term in ['revenue', 'price', 'cost', 'value', 'amount', 'income', 'expense']):\n",
    "                    currency_cols.append(col)\n",
    "        \n",
    "        # Format the currency columns\n",
    "        for col in currency_cols:\n",
    "            if col in formatted_df.columns:\n",
    "                # Check if column is numeric\n",
    "                col_type = formatted_df[col].dtype\n",
    "                is_numeric = any(num_type in str(col_type) for num_type in ['Int', 'Float', 'int', 'float'])\n",
    "                \n",
    "                if is_numeric:\n",
    "                    # Format currency values with specified symbol and decimal places\n",
    "                    formatted_df = formatted_df.with_columns(\n",
    "                        pl.col(col).map_elements(\n",
    "                            lambda x: f\"{currency_symbol}{x:.{decimal_places}f}\" if x is not None else \"N/A\",\n",
    "                            return_dtype=pl.Utf8\n",
    "                        ).alias(col)\n",
    "                    )\n",
    "    \n",
    "    # Format other numeric columns to have consistent decimal places\n",
    "    for col in formatted_df.columns:\n",
    "        if col not in currency_cols:\n",
    "            col_type = formatted_df[col].dtype\n",
    "            is_numeric = any(num_type in str(col_type) for num_type in ['Int', 'Float', 'int', 'float'])\n",
    "            \n",
    "            if is_numeric:\n",
    "                formatted_df = formatted_df.with_columns(\n",
    "                    pl.col(col).map_elements(\n",
    "                        lambda x: f\"{x:.{decimal_places}f}\" if isinstance(x, float) else str(x),\n",
    "                        return_dtype=pl.Utf8\n",
    "                    ).alias(col)\n",
    "                )\n",
    "    \n",
    "    # Generate the markdown table\n",
    "    # Format the header\n",
    "    header = \"| \" + \" | \".join(formatted_df.columns) + \" |\"\n",
    "    \n",
    "    # Format the separator\n",
    "    separator = \"| \" + \" | \".join([\"---\" for _ in formatted_df.columns]) + \" |\"\n",
    "    \n",
    "    # Format each row\n",
    "    rows = []\n",
    "    for row in formatted_df.iter_rows(named=True):\n",
    "        formatted_row = \"| \" + \" | \".join([str(row[col]) for col in formatted_df.columns]) + \" |\"\n",
    "        rows.append(formatted_row)\n",
    "    \n",
    "    # Combine all parts\n",
    "    markdown_table = \"\\n\".join([header, separator] + rows)\n",
    "    \n",
    "    # Auto-generate description if not provided\n",
    "    if description is None:\n",
    "        # Try to identify what the data is about\n",
    "        col_list = list(formatted_df.columns)\n",
    "        \n",
    "        # Extract key column types\n",
    "        group_cols = [col for col in col_list if col not in (currency_cols or [])]\n",
    "        \n",
    "        if group_cols:\n",
    "            description = f\"I'm sharing data grouped by {', '.join(group_cols)}.\"\n",
    "            if currency_cols:\n",
    "                description += f\" The table includes {', '.join(currency_cols)} values.\"\n",
    "        else:\n",
    "            description = \"I'm sharing the following data for analysis.\"\n",
    "    \n",
    "    # Add a paragraph with some \"expert\" context to make the returned insights more helpful. \n",
    "    expert_context = [\n",
    "        \"Target demo and all demo impressions should not be related to each other, as target demo is simply a subset of all demos\",\n",
    "        \"While the overnight (ON) is cheap, it does not scale well. Be cautious in suggesting moving dollars to the overnight\",\n",
    "        \"Be hyper-vigilant to sample size. We do not want to make optimizations using only a few instances of a data point\",\n",
    "        \"Avoid providing generic advice such as 'move budget to stations with low CPMs.' Be descriptive in your suggestions and make refernce to specifics\"\n",
    "    ]\n",
    "    expert_context = \"Here is some expert context to help you:\\n\" + \"\\n\".join([f\"{i+1}. {item}\" for i, item in enumerate(expert_context)])\n",
    "\n",
    "    # Default analysis request if not provided\n",
    "    if analysis_request is None:\n",
    "        analysis_request = [\n",
    "            \"Key insights and patterns in the data\",\n",
    "            \"Notable outliers or anomalies\",\n",
    "            \"Interpretations of the trends observed\",\n",
    "            \"Strategic recommendations based on this data\"\n",
    "        ]\n",
    "    \n",
    "    # Format the analysis request\n",
    "    if isinstance(analysis_request, list):\n",
    "        formatted_request = \"Please provide:\\n\" + \"\\n\".join([f\"{i+1}. {item}\" for i, item in enumerate(analysis_request)])\n",
    "    else:\n",
    "        formatted_request = analysis_request\n",
    "    \n",
    "    # Construct the full prompt\n",
    "    prompt = f\"\"\"# {title}\n",
    "\n",
    "{description}\n",
    "\n",
    "{markdown_table}\n",
    "\n",
    "{expert_context}\n",
    "\n",
    "{formatted_request}\n",
    "\"\"\"\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee3066c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = generate_table_prompt(data)\n",
    "#display(Markdown(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6211339e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error calling Gemini API: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}\n"
     ]
    },
    {
     "ename": "ServerError",
     "evalue": "503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mServerError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m response = \u001b[43mmake_gemini_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m display(Markdown(response))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/development/ai/ai-research/chiefai/ai.py:87\u001b[39m, in \u001b[36mmake_gemini_request\u001b[39m\u001b[34m(request_text, model, temperature, max_output_tokens, top_p, top_k, api_key, return_full_response)\u001b[39m\n\u001b[32m     84\u001b[39m     params[\u001b[33m\"\u001b[39m\u001b[33mtop_k\u001b[39m\u001b[33m\"\u001b[39m] = top_k\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# Make the request with parameters directly included\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# Return full response or just text based on parameter\u001b[39;00m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_full_response:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/development/ai/lib/python3.13/site-packages/google/genai/models.py:5202\u001b[39m, in \u001b[36mModels.generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   5200\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m remaining_remote_calls_afc > \u001b[32m0\u001b[39m:\n\u001b[32m   5201\u001b[39m   i += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m5202\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5203\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\n\u001b[32m   5204\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5205\u001b[39m   logger.info(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mAFC remote call \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is done.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   5206\u001b[39m   remaining_remote_calls_afc -= \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/development/ai/lib/python3.13/site-packages/google/genai/models.py:4178\u001b[39m, in \u001b[36mModels._generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   4175\u001b[39m request_dict = _common.convert_to_dict(request_dict)\n\u001b[32m   4176\u001b[39m request_dict = _common.encode_unserializable_types(request_dict)\n\u001b[32m-> \u001b[39m\u001b[32m4178\u001b[39m response_dict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4179\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[32m   4180\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._api_client.vertexai:\n\u001b[32m   4183\u001b[39m   response_dict = _GenerateContentResponse_from_vertex(\n\u001b[32m   4184\u001b[39m       \u001b[38;5;28mself\u001b[39m._api_client, response_dict\n\u001b[32m   4185\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/development/ai/lib/python3.13/site-packages/google/genai/_api_client.py:755\u001b[39m, in \u001b[36mBaseApiClient.request\u001b[39m\u001b[34m(self, http_method, path, request_dict, http_options)\u001b[39m\n\u001b[32m    745\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest\u001b[39m(\n\u001b[32m    746\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    747\u001b[39m     http_method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    750\u001b[39m     http_options: Optional[HttpOptionsOrDict] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    751\u001b[39m ) -> Union[BaseResponse, Any]:\n\u001b[32m    752\u001b[39m   http_request = \u001b[38;5;28mself\u001b[39m._build_request(\n\u001b[32m    753\u001b[39m       http_method, path, request_dict, http_options\n\u001b[32m    754\u001b[39m   )\n\u001b[32m--> \u001b[39m\u001b[32m755\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    756\u001b[39m   json_response = response.json\n\u001b[32m    757\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m json_response:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/development/ai/lib/python3.13/site-packages/google/genai/_api_client.py:684\u001b[39m, in \u001b[36mBaseApiClient._request\u001b[39m\u001b[34m(self, http_request, stream)\u001b[39m\n\u001b[32m    676\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    677\u001b[39m   response = \u001b[38;5;28mself\u001b[39m._httpx_client.request(\n\u001b[32m    678\u001b[39m       method=http_request.method,\n\u001b[32m    679\u001b[39m       url=http_request.url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    682\u001b[39m       timeout=http_request.timeout,\n\u001b[32m    683\u001b[39m   )\n\u001b[32m--> \u001b[39m\u001b[32m684\u001b[39m   \u001b[43merrors\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAPIError\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    685\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[32m    686\u001b[39m       response.headers, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response.text]\n\u001b[32m    687\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/development/ai/lib/python3.13/site-packages/google/genai/errors.py:103\u001b[39m, in \u001b[36mAPIError.raise_for_response\u001b[39m\u001b[34m(cls, response)\u001b[39m\n\u001b[32m    101\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ClientError(status_code, response_json, response)\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[32m500\u001b[39m <= status_code < \u001b[32m600\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ServerError(status_code, response_json, response)\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    105\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(status_code, response_json, response)\n",
      "\u001b[31mServerError\u001b[39m: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}"
     ]
    }
   ],
   "source": [
    "response = make_gemini_request(request_text=prompt)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2f9d88",
   "metadata": {},
   "source": [
    "## Hybrid Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fbb228",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chiefai.ai import analyze_campaign_data\n",
    "import polars as pl\n",
    "\n",
    "# Run analysis\n",
    "results = analyze_campaign_data(\n",
    "    data,\n",
    "    temperature=0.3,\n",
    "    min_sample_size=5,\n",
    "    confidence_level=0.95\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
